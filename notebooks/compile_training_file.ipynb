{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e14f742",
   "metadata": {},
   "source": [
    "# Compile Training Chunks into a Single File\n",
    "Read all Parquet chunk files from `training_chunks/`, concatenate them into a single DataFrame, and save as one consolidated Parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6927b3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 31 chunk files\n",
      "Combined DataFrame shape: (4405108, 76)\n",
      "Columns: ['ch1_LS', 'ch1_MFL', 'ch1_MSR', 'ch1_WAMP', 'ch1_ZC', 'ch1_RMS', 'ch1_IAV', 'ch1_DASDV', 'ch1_VAR', 'ch2_LS', 'ch2_MFL', 'ch2_MSR', 'ch2_WAMP', 'ch2_ZC', 'ch2_RMS', 'ch2_IAV', 'ch2_DASDV', 'ch2_VAR', 'ch3_LS', 'ch3_MFL', 'ch3_MSR', 'ch3_WAMP', 'ch3_ZC', 'ch3_RMS', 'ch3_IAV', 'ch3_DASDV', 'ch3_VAR', 'ch4_LS', 'ch4_MFL', 'ch4_MSR', 'ch4_WAMP', 'ch4_ZC', 'ch4_RMS', 'ch4_IAV', 'ch4_DASDV', 'ch4_VAR', 'ch5_LS', 'ch5_MFL', 'ch5_MSR', 'ch5_WAMP', 'ch5_ZC', 'ch5_RMS', 'ch5_IAV', 'ch5_DASDV', 'ch5_VAR', 'ch6_LS', 'ch6_MFL', 'ch6_MSR', 'ch6_WAMP', 'ch6_ZC', 'ch6_RMS', 'ch6_IAV', 'ch6_DASDV', 'ch6_VAR', 'ch7_LS', 'ch7_MFL', 'ch7_MSR', 'ch7_WAMP', 'ch7_ZC', 'ch7_RMS', 'ch7_IAV', 'ch7_DASDV', 'ch7_VAR', 'ch8_LS', 'ch8_MFL', 'ch8_MSR', 'ch8_WAMP', 'ch8_ZC', 'ch8_RMS', 'ch8_IAV', 'ch8_DASDV', 'ch8_VAR', 'label', 'user', 'sample_id', 'window_start']\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "pinch        734363\n",
      "noGesture    734239\n",
      "fist         734237\n",
      "waveOut      734191\n",
      "open         734097\n",
      "waveIn       733981\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saved consolidated training set to: C:\\Users\\antol\\Documents\\Documenti\\UNI_sant anna\\data mining\\EMG-EPN612-PROJECT\\training_set.parquet\n",
      "File size: 1136.8 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to the chunks directory\n",
    "CHUNKS_DIR = Path(\"../training_chunks\")\n",
    "OUTPUT_FILE = Path(\"../training_set.parquet\")\n",
    "\n",
    "# Read and concatenate all chunk files\n",
    "chunk_files = sorted(CHUNKS_DIR.glob(\"chunk_*.parquet\"))\n",
    "print(f\"Found {len(chunk_files)} chunk files\")\n",
    "\n",
    "df = pd.concat([pd.read_parquet(f) for f in chunk_files], ignore_index=True)\n",
    "print(f\"Combined DataFrame shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"\\nLabel distribution:\\n{df['label'].value_counts()}\")\n",
    "\n",
    "# Save as a single Parquet file\n",
    "df.to_parquet(OUTPUT_FILE, index=False)\n",
    "print(f\"\\nSaved consolidated training set to: {OUTPUT_FILE.resolve()}\")\n",
    "print(f\"File size: {OUTPUT_FILE.stat().st_size / (1024**2):.1f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emg-epn612",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
